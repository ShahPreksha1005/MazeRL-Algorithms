# MazeRL-Algorithms

## Overview
This repository showcases the implementation of three key Reinforcement Learning (RL) algorithms for solving a maze navigation problem:
- **Dynamic Programming (DP)**: Uses full environment knowledge to iteratively compute the optimal policy.
- **Monte Carlo (MC) Learning**: Learns from complete episodes of interaction with the environment.
- **Temporal Difference (TD) Learning**: Learns incrementally from incomplete episodes by bootstrapping current estimates.

The goal is to analyze and compare the effectiveness of these RL methods in a controlled maze environment.

## Objective
- Implement and compare different RL algorithms in a maze navigation problem.
- Gain practical experience in RL-based decision-making and policy optimization.
- Evaluate agent performance based on reward maximization and learning efficiency.

## Maze Environment
The maze consists of:
- **Obstacles**: Restricted areas where the agent cannot move.
- **Starting locations**: Possible initial positions of the agent.
- **Absorbing states (goal positions)**: The agent aims to reach these states for rewards.
- **Rewards**: Assigned based on reaching goals or making invalid moves.

## Implemented RL Algorithms
### 1. Dynamic Programming (DP)
- Uses **Value Iteration** and **Policy Iteration** to compute optimal policies.
- Requires a known model of the environment.
  
### 2. Monte Carlo (MC) Learning
- Uses **episodic updates** based on complete trajectories.
- Learns value functions without prior knowledge of transition probabilities.
  
### 3. Temporal Difference (TD) Learning
- Uses **bootstrapping** to update value estimates after each step.
- Implements **TD(0)** for incremental learning.

## Key Features
- **Custom Maze Environment**: Programmatically defined maze structure with obstacles and goals.
- **Algorithm Comparisons**: Evaluates policies generated by DP, MC, and TD methods.
- **Visualization**: Graphical representations of policies and value functions.
- **Performance Metrics**: Analyzes learning efficiency and cumulative rewards.

## Installation & Setup
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/MazeRL-Algorithms.git
   cd MazeRL-Algorithms
   ```
2. Install dependencies:
   ```bash
   pip install numpy matplotlib
   ```
3. Run the notebook:
   ```bash
   jupyter notebook maze_rl.ipynb
   ```

## Results & Analysis
- **DP achieves optimal solutions quickly** but requires a known transition model.
- **MC effectively learns optimal policies** but needs more iterations for convergence.
- **TD balances between DP and MC**, learning efficiently in unknown environments.

## Future Enhancements
- Implement **Deep Q-Learning (DQN)** for function approximation.
- Test on **larger and dynamic maze environments**.
- Optimize hyperparameters for improved convergence.

## References
- Sutton & Barto, "Reinforcement Learning: An Introduction."
- Richard Bellman, "Dynamic Programming."

---
